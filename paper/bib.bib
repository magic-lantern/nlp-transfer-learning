@misc{howard_universal_2018,
	title = {Universal {Language} {Model} {Fine}-tuning for {Text} {Classification}},
	url = {http://arxiv.org/abs/1801.06146},
	abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
	urldate = {2018-06-19},
	journal = {arXiv:1801.06146 [cs, stat]},
	author = {Howard, Jeremy and Ruder, Sebastian},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.06146},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language, Statistics - Machine Learning}
}

@misc{radford_improving_2018,
	title = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
	language = {en},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year = {2018},
	pages = {12},
	annote = {https://blog.openai.com/language-unsupervised/},
	annote = {https://github.com/openai/finetune-transformer-lm}
}

@misc{radford_language_2019,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	language = {en},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year = {2019},
	pages = {24},
	annote = {https://blog.openai.com/better-language-models/},
	annote = {https://github.com/openai/gpt-2}
}

@article{Vincent2019Feb,
	author = {Vincent, James},
	title = {{OpenAI unveils multitalented AI that writes, translates, and slanders}},
	journal = {Verge},
	year = {2019},
	month = {Feb},
	publisher = {The Verge},
	url = {https://www.theverge.com/2019/2/14/18224704/ai-machine-learning-language-models-read-write-openai-gpt2}
}

@article{Piper2019Feb,
	author = {Piper, Kelsey},
	title = {{OpenAI{'}s new language system writes really great fake news}},
	journal = {Vox},
	year = {2019},
	month = {Feb},
	publisher = {Vox},
	url = {https://www.vox.com/future-perfect/2019/2/14/18222270/artificial-intelligence-open-ai-natural-language-processing}
}


@article{johnson_mimic-iii_2016,
	title = {{MIMIC}-{III}, a freely accessible critical care database},
	volume = {3},
	copyright = {2016 Nature Publishing Group},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201635},
	doi = {10.1038/sdata.2016.35},
	abstract = {MIMIC-III (‘Medical Information Mart for Intensive Care’) is a large, single-center database comprising information relating to patients admitted to critical care units at a large tertiary care hospital. Data includes vital signs, medications, laboratory measurements, observations and notes charted by care providers, fluid balance, procedure codes, diagnostic codes, imaging reports, hospital length of stay, survival data, and more. The database supports applications including academic and industrial research, quality improvement initiatives, and higher education coursework.},
	language = {en},
	urldate = {2019-07-26},
	journal = {Scientific Data},
	author = {Johnson, Alistair E. W. and Pollard, Tom J. and Shen, Lu and Lehman, Li-wei H. and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G.},
	month = may,
	year = {2016},
	pages = {160035}
}

@InProceedings{ mckinney-proc-scipy-2010,
	author    = { Wes McKinney },
	title     = { Data structures for statistical computing in python },
	booktitle = { Proceedings of the 9th Python in Science Conference },
	pages     = { 51 - 56 },
	year      = { 2010 },
	editor    = { St\'efan van der Walt and Jarrod Millman }
}

@article{Merity2016Sep,
	author = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
	title = {{Pointer Sentinel Mixture Models}},
	journal = {arXiv},
	year = {2016},
	month = {Sep},
	eprint = {1609.07843},
	url = {https://arxiv.org/abs/1609.07843}
}

@article{Merity2017Aug,
	author = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
	title = {{Regularizing and Optimizing LSTM Language Models}},
	journal = {arXiv},
	year = {2017},
	month = {Aug},
	eprint = {1708.02182},
	url = {https://arxiv.org/abs/1708.02182}
}

@article{Bengio:2003:NPL:944919.944966,
 author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Janvin, Christian},
 title = {A Neural Probabilistic Language Model},
 journal = {J. Mach. Learn. Res.},
 issue_date = {3/1/2003},
 volume = {3},
 month = mar,
 year = {2003},
 issn = {1532-4435},
 pages = {1137--1155},
 numpages = {19},
 url = {http://dl.acm.org/citation.cfm?id=944919.944966},
 acmid = {944966},
 publisher = {JMLR.org},
} 

@article{Devlin2018Oct,
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
	journal = {arXiv},
	year = {2018},
	month = {Oct},
	eprint = {1810.04805},
	url = {https://arxiv.org/abs/1810.04805}
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2019-03-13},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv\:1706.03762 PDF:/Users/seth/Zotero/storage/VZD76C5N/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf}
}