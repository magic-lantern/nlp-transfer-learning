\documentclass{amia}
\usepackage{graphicx}
\usepackage[labelfont=bf]{caption}
\usepackage[superscript,nomove]{cite}
\usepackage{color}
\usepackage{url}                      % simple URL typesetting
\usepackage{hyperref}                 % add color to links and make them clickable
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=blue
}
\usepackage[T1]{fontenc}              % use 8-bit T1 fonts; support emdash

\begin{document}

\title{Learning from others: Transfer learning in clinical NLP}

\author{Seth Russell, MS$^{1}$,
    Tellen D. Bennett, MD, MS$^{1,2}$,
    Debashis Ghosh, PhD$^{1,3}$}

\institutes{
    $^1$CU Data Science to Patient Value (D2V), Anschutz Medical Campus, Aurora, CO;
    $^2$Pediatric Critical Care, University of Colorado School of Medicine, Aurora, CO;
    $^3$Biostatistics and Informatics, Colorado School of Public Health, Aurora, CO
}

\maketitle

% not enough space to fit abstract, but keeping it in for version control purposes
% \noindent{\bf Abstract}

% \textit{The use of language models in deep learning natural language processing has shown recent success in many general English language benchmarks. Through unsupervised creation of language models, performance on wide variety of problems is improved as well as reducing training time. In this work we explore the effectiveness of the fine-tuning language models for transfer learning applied to the medical domain. Our language modeling and retrospective classifications tasks were carried out using data from MIMIC.}

\section*{Introduction}

Language models built in an unsupervised fashion have recently improved the performance of natural language processing (NLP) machine learning models.\cite{howard_universal_2018} This form of transfer learning is analogous to successes seen with transfer learning in domains such as imaging. Building upon this work and using massive data sets and compute power, others have recently established new records on various benchmark tests for state of the art NLP.\cite{radford_language_2019}

To date, the successes seen with transfer learning in NLP have focused on general English non-medical text. The contribution of this work is to demonstrate the effectiveness of transfer learning applied to the medical domain. NLP transfer learning is similar to transfer learning in computer vision, which a general computer vision model can recognize common image features such as lines, circles, edges, and composite patterns. Analogously, a language model is trained to predict and recognize language features at both the word and sentence level. Language models are trained in an unsupervised fashion by having the model predict the next word in a sentence of text given previous words. Source code for this work is publicly available at \footnote{\url{https://github.com/magic-lantern/nlp-transfer-learning}}.

\section*{Methods}

We conducted several experiments to evaluate the usefulness of transfer learning in clinical NLP. We used the Medical Information Mart for Intensive Care (MIMIC) III \cite{johnson_mimic-iii_2016} data set, which contains both clinical notes as well as a wide array of structured data. We randomly sampled MIMIC clinical notes without replacement with different seeds for the language model versus classification tasks to reduce overlap.

We first retrained an existing language model derived from Wikipedia articles (WT-103) \cite{Merity2016Sep} with clinical text. The neural network architecture chosen was based on Averaged Stochastic Gradient Weight-Dropped LSTM (AWD-LSTM) \cite{Merity2017Aug}. We prepared the text by tokenizing (identifying separate words, numbers, symbols) and then converting each token to a unique integer (XXX Seth is this true?). Special tokens were inserted into the sentences to preserve the original text structure such as capitalization, repeated words, and words not in the vocabulary list. No stemming, lemmatization, nor stop word removal was performed.

Once the language model was fine-tuned against a random sample of MIMIC clinical notes, we used the language model for 2 separate classification tasks. We evaluated performance using accuracy and F1 weighted by the number of true instances for each label. The first classification task was to predict the note type (e.g. XXXX). The second classification task was to predict length of stay (LOS) in days for admitted patients with clinical notes, based on notes from their first day of stay, binned into 0 through 9 days and > 9 days. For comparison, we also performed the classification tasks with no language model fine-tuning. All experiments were carried out on a single virtual machine with 6 virtual CPUs, 32GB RAM, 200GB of virtualized disk space, and an NVIDIA Tesla P40 GPU with 24GB of GPU RAM. Each experiment used only 10 training epochs.

\section*{Results}

For the language model, we selected a random 10\% sample of all notes and used 60,000 as the vocabulary size. We found 24.4\% overlap in tokens between WT-103 language model and MIMIC language model; e.g. words such as 'circumference' and 'purple' exist in both. Words unique to the MIMIC language model include examples such as 'transgastric', 'dysregulation', and 'bronchiectasis'. With 1 frozen (adjusting weights of last layer only) and 9 unfrozen (adjusting weights of all layers) epochs, model accuracy improves from 55.9\% to 67.7\%  at predicting the next token in a clinical note. The resulting model is 550MB in size and was trained in approximately 12 hours. Each classifier was trained in under 6 hours.

For the note type classifier, we selected 10\% of the notes to train and test (XXX this is a bit vague). We found 9.9\% overlap between notes used for training and validating the language model and training and validating the description classifier. 1,708 distinct note type values were present, most labeled generically as 'Report' (54.5\%), with a long tail of decreasing frequency including 695 note types only used once. Using transfer learning and a MIMIC fine-tuned language model, accuracy improves from 90.7\% to 95.7\% and weighted F1 from 0.89 to 0.95 over the 10 epochs of training. By comparison, using a generic language model, accuracy improves only from 86.3\% to XX.X\% and weighted F1 from 0.84 to 0.XX.

For the length of stay (LOS) classifier, there was no overlap from the collection of notes used to train and validate the language model. The LOS sample was comprised of 33,914 clinical notes. LOS for the patient population with notes ranged from 0 to 206, with median of 6 and mode of 4. After binning, 30.4\% had LOS > 9 days. The next most common LOS was 4 days (10.3\%) and the least common LOS was 0 days (3.7\%). Using transfer learning and a MIMIC fine-tuned language model, accuracy improves from XX.X\% to XX.X\% and weighted F1 from 0.XX to 0.XX over the 10 epochs of training. Using a generic language model, accuracy improves only from XX.X\% to XX.X\%, and weighted F1 from 0.XX to 0.XX. 

\section*{Discussion}

Transfer learning using language models improves the performance of NLP in the medical domain. Language models fine-tuned for medical text provide greater performance improvement than generic langugage models. Interestingly, this performance improvement was obtained with only 10 training epochs on a single GPU virtual machine with capabilities that would be affordable to many investigative teams. While larger and deeper neural networks trained for 1000s of hours will likely achieve better performance than the network architectures used here, our methods can be trained relatively rapidly and still achieve high accuracy. Increased accuracy with these specific models can be achieved through more training epochs, though with diminishing returns. Although this work was done with the MIMIC data set, many health care settings will have notes that vary from MIMIC, but should be able to employ a similar technique of fine-tuning for their vocabulary and corpora. Future work in this area could include the use of bi-directional language models, more advanced network architectures, and additional training epochs.

\section*{Conclusion}

We have shown that recent advances in NLP transfer learning do work with medical text. Adjusting vocabularies and weights from a general language model works improves performance for the classification tasks chosen here. While more advanced network architectures or more hardware could improve results, the transfer learning techniques used here do achieve good accuracy.

\bibliography{bib}
\bibliographystyle{vancouver}

\end{document}

