\documentclass{amia}
\usepackage{graphicx}
\usepackage[labelfont=bf]{caption}
\usepackage[superscript,nomove]{cite}
\usepackage{color}
\usepackage{url}                      % simple URL typesetting
\usepackage{hyperref}                 % add color to links and make them clickable
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=blue
}
\usepackage[T1]{fontenc}              % use 8-bit T1 fonts; support emdash

\begin{document}


\title{Learning from others: Transfer learning in medical NLP}

\author{Seth Russell, MS$^{1}$, Firstname B. Lastname, Degrees$^{2}$}

\institutes{
    $^1$CU Data Science to Patient Value (D2V), Anschutz Medical Campus, Aurora, CO; $^2$Institution, City, State, Country (if applicable)\\
}

\maketitle

\noindent{\bf Abstract}

\textit{Abstract text goes here (if enough room),  justified and in italics.  50 - 75 words for podium abstract.}

\section*{Introduction}

In the natural language processing (NLP) machine learning domain, great success has been found recently through the creation of language models built in an unsupervised fashion. Derived from successes seen with transfer learning in domains such as imaging, one early publication \cite{howard_universal_2018} showed success with NLP transfer learning. Building upon this work primarily through the use of massive data sets and compute power \cite{radford_improving_2018,radford_language_2019} resulted in new records on various benchmark tests for state of the art NLP.

To date, the successes seen with transfer learning in NLP have focused on general English non-medical text. The contribution of this work is to demonstrate the effectiveness of the transfer learn techniques

\section*{Methods}

For this study, we have used the Medical Information Mart for Intensive Care (MIMIC) III \cite{johnson_mimic-iii_2016} data set which contains both clinical notes as well as a wide array of more structured data. Although the MIMIC dataset is comprised primarily of inpatient ICU stays, some outpatient notes are included. The patient population includes 38,597 distinct adult patients admitted between 2001 and 2012 as well as 7870 neonates admitted between 2001 and 2008. Our cohort from MIMIC consisted of a random 10\% sample of clinical notes using the Pandas Python library \cite{mckinney-proc-scipy-2010}. All experiments were carried out on a virtual machine with 6 virtual CPUs, 32GB RAM, 200GB of virtualized disk space, and an Nvidia Tesla P100 GPU with 16GB of GPU RAM. 

We conducted several experiments to evaluate the usefulness of transfer learning in clinical NLP. The basis for transfer learning in this clinical NLP work is the use of a language model. Similar to transfer learning in computer vision, a computer vision general model can recognize general image features such as lines, circles, and other patterns, a language model is trained to predict and recognize language features in terms of probabilities at both the word and sentence level \cite{Bengio:2003:NPL:944919.944966}. Language models are trained in an unsupervised fashion by having the model predict the next word in a batch of text given previous words.

Our first experiment was to take an existing language model derived from Wikipedia articles (WT-103) \cite{Merity2016Sep} and retrain it with clinical text. The neural network architecture chosen was based on ASGD Weight-Dropped LSTM (AWD-LSTM) \cite{Merity2017Aug} comprised of layers? 

NOTES:
First - fine tune langauge model
Next - test performance of language model on categorical tasks: 1) description 2) length of stay

\section*{Results}

For the language model, we use 60,000 as the vocabulary size. There are 14,655 tokens that overlap between WT-103 language model and MIMIC language model; e.g. words such as 'circumference' and 'purple' exist in both. Words unique to the MIMIC langauge model include examples such as 'transgastric', 'dysregulation', and 'bronchiectasis'. Overall there is about 25\% overlap in vocabulary.

\section*{Discussion}

Some discussion text here

Future work - bi-directional language model ala BERT; transformer based architecture also shown impressive results but requires much more training/GPU resources...
Current work can be performed on a single GPU

\section*{Another Major Heading and References}
This sentence and has two reference citations\cite{ref1,ref2}.

More text of an additional paragraph, with a figure reference (Figure ~\ref{fig1}) and a figure inside a Word text box
below.  Figures need to be placed as close to the corresponding text as possible and not extend beyond one page.\\
\begin{figure}[h!]
\centering
\includegraphics[scale=1]{figures/figure1.png}
\caption{Total allergy alerts, overridden alerts, or drug order cancelled.}
\label{fig1}
\end{figure}

This paragraph contains a reference to a table just below (Table 1).  All tables need to be placed as close to the 
corresponding text as possible, But each individual table should be on one page and not extend to multiple pages
 unless labeled as ``��Continued"��.

\begin{table}[h]
\centering
\caption{Submission type, abstract length, and page length maximum for AMIA submissions.}
  \begin{tabular}{|l|l|l|}
  \hline
    \textbf{Submission Type}    & \textbf{Abstract Length}  & \textbf{Page Length Maximum**} \\ \hline
    Paper  & 125-150 words  & Ten   \\ \hline
    Student Paper  & 125-150 words  & Ten \\ \hline
    Poster  &50-75 words*   & One \\ \hline
    Podium  Abstract & 50-75 words*  & Two \\ \hline
    Panel   &150-200 words  & Three \\ \hline
    System Demonstrations    &150-200 words  & One \\ \hline
  \end{tabular}
\end{table}
*: All podium abstract and poster submissions must have a brief (50-75 words) abstract. The abstract does NOT have to be part of the document, but must be entered on the submission website in the Abstract box in Step 2.

**: \textcolor{red}{If your submission is longer than what is specified below, it will be rejected without review.}

This is another paragraph.

\section*{Conclusion}
Your conclusion goes at the end, followed by References, which must follow the Vancouver Style (see: www.icmje.org/index.html).  References begin below with a header that is centered.  Only the first word of an article title is capitalized in the References. 

\section*{Project TO DO:}

\begin{itemize}
	\item Create Conda environment.yml that allows users to setup requirements for project
\end{itemize}

\bibliography{bib}
\bibliographystyle{vancouver}

\end{document}

