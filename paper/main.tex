\documentclass{amia}
\usepackage{graphicx}
\usepackage[labelfont=bf]{caption}
\usepackage[superscript,nomove]{cite}
\usepackage{color}
\usepackage{url}                      % simple URL typesetting
\usepackage{hyperref}                 % add color to links and make them clickable
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=blue
}
\usepackage[T1]{fontenc}              % use 8-bit T1 fonts; support emdash

\begin{document}

\title{Learning from others: Transfer learning in clinical NLP}

\author{Seth Russell, MS$^{1}$,
    Tellen D. Bennett, MD, MS$^{1,2}$,
    Debashis Ghosh, PhD$^{1,3}$}

\institutes{
    $^1$CU Data Science to Patient Value (D2V), Anschutz Medical Campus, Aurora, CO;
    $^2$Pediatric Critical Care, University of Colorado School of Medicine, Aurora, CO;
    $^3$Biostatistics and Informatics, Colorado School of Public Health, Aurora, CO
}

\maketitle

\section*{Introduction}

In the natural language processing (NLP) machine learning domain, great success has been found recently through the creation of language models built in an unsupervised fashion. Derived from successes seen with transfer learning in domains such as imaging, one early publication \cite{howard_universal_2018} showed success with NLP transfer learning. Building upon this work primarily through the use of massive data sets and compute power \cite{radford_language_2019} has resulted in new records on various benchmark tests for state of the art NLP.

To date, the successes seen with transfer learning in NLP have focused on general English non-medical text. The contribution of this work is to demonstrate the effectiveness of the transfer learn techniques applied to the medical domain. NLP transfer learning is similar to transfer learning in computer vision where a general computer vision model can recognize common image features such as lines, circles, edges, and composite patterns. In NLP transfer learning, a language model is trained to predict and recognize language features in terms of probabilities at both the word and sentence level. Language models are trained in an unsupervised fashion by having the model predict the next word in a sentence of text given previous words. Source code for this work is publicly available at \footnote{\url{https://github.com/magic-lantern/nlp-transfer-learning}}.

\section*{Methods}

For this retrospective study we conducted several experiments to evaluate the usefulness of transfer learning in clinical NLP. We have used the Medical Information Mart for Intensive Care (MIMIC) III \cite{johnson_mimic-iii_2016} data set which contains both clinical notes as well as a wide array of structured data. Our cohort from MIMIC consisted of a random sample without replacement of clinical notes with different seeds for the language model vs classification tasks to reduce overlap.

Our first experiment was to take an existing language model derived from Wikipedia articles (WT-103) \cite{Merity2016Sep} and retrain it with clinical text. The neural network architecture chosen was based on Averaged Stochastic Gradient Weight-Dropped LSTM (AWD-LSTM) \cite{Merity2017Aug}. Text was prepared by tokenization (identifying separate words, numbers, symbols) and then conversion of those tokens to a number. Special tokens are inserted into the sentences to preserve the original text structure such as capitalization, repeated words, and identification of words not in the vocabulary list. No stemming, lemmatization, nor stop word removal was performed.

Once the language model was fine-tuned against a random sample of MIMIC clinical notes, we used the language model for 2 separate classification tasks. Accuracy and F1 weighted by the number of true instances for each label are reported. The first classification task was to predict the note description. The second classification task was to predict length of stay (LOS) in days for admitted patients with clinical notes, based on notes from their first day of stay, binned into 0 through 9 days and > 9 days. For comparison, we also performed the classification tasks with no language model fine-tuning to evaluate accuracy over 10 epochs of training. All experiments were carried out on a single virtual machine with 6 virtual CPUs, 32GB RAM, 200GB of virtualized disk space, and an NVIDIA Tesla P40 GPU with 24GB of GPU RAM.

\section*{Results}

For the language model, we used 60,000 as the vocabulary size. There is a 24.4\% overlap in tokens between WT-103 language model and MIMIC language model; e.g. words such as 'circumference' and 'purple' exist in both. Words unique to the MIMIC language model include examples such as 'transgastric', 'dysregulation', and 'bronchiectasis'. With 1 frozen (adjusting weights of last layer only) and 9 unfrozen (adjusting weights of all layers) epochs, the model goes from 55.9\% accuracy to 67.7\% accuracy at predicting the next token in a clinical note. The resulting model is 550MB in size.

For the description classifier, we selected 10\% of the clinical notes to train and test our classifier. There was a 9.91\% overlap between notes used for training and validating the language model and training and validating the description classifier. There are 1,708 distinct description values present, with most being labeled generically as 'Report' (54.5\%), and a long tail of decreasing frequency including 695 descriptions that are only used once. When starting with a MIMIC fine-tuned language model accuracy goes from 90.7\% to 9X.X\%, and weighted F1 from 0.88 to 0.XX, with X epochs of training. By comparison, starting with a non-MIMIC-fine-tuned language model, accuracy starts at 86.5\% and improves to to XX.X\%, and weighted F1 from XX to XX with X epochs of training.

For the length of stay (LOS) classifier, there was no overlap from the collection of notes used to train and validate the language model. The LOS sample was comprised of 33,914 notes from admitted patients. LOS for the patient population with notes ranged from 0 to 206, with median of 6 and mode of 4. After binning, > 9 accounts for 30.4\%, with next being 4 days 10.3\%, and least frequent of 0 days at 3.7\%. When starting with a MIMIC fine-tuned language model accuracy goes from XX.X\% to XX.X\%, and weighted F1 from 0.XX to 0.XX, with X epochs of training. By comparison, starting with a non-fine-tuned language model, accuracy goes from XX.X\% to XX.X\%, and weighted F1 from 0.XX to 0.XX with X epochs of training.

\section*{Discussion}

Language models and transfer learning does speed up the training of AWD-LSTM networks. While larger and deeper neural networks trained for 1000s of hours will achieve better accuracy/F1 than the network architectures used here, our methods can be trained relatively rapidly while achieving high accuracy. These experiments were performed on a single GPU with training times for the language model at about 12 hours and each of the classifiers being trained in under 6 hours. Increased accuracy with these specific models can be achieved through more training epochs, though with diminishing returns. While this work was done with the MIMIC data set, every health care setting will have notes that vary from MIMIC, but should be able to employ a similar technique of fine-tuning for their vocabulary and corpora. Future work in this area could include the use of bi-directional language models, more advanced network architectures, and additional training epochs.

\section*{Conclusion}

Using the MIMIC clinical notes, we have shown that recent advances in NLP transfer learning do work with medical text. Adjusting vocabularies and weights from a general language model works well for the classification tasks chosen here. While more advanced network architectures or more hardware could improve results, the transfer learning techniques used here do achieve good accuracy.

\bibliography{bib}
\bibliographystyle{vancouver}

\end{document}

