\documentclass{amia}
\usepackage{graphicx}
\usepackage[labelfont=bf]{caption}
\usepackage[superscript,nomove]{cite}
\usepackage{color}
\usepackage{url}                      % simple URL typesetting
\usepackage{hyperref}                 % add color to links and make them clickable
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=blue
}
\usepackage[T1]{fontenc}              % use 8-bit T1 fonts; support emdash

\begin{document}

\title{Learning from others: Transfer learning in clinical NLP}

\author{Seth Russell, MS$^{1}$,
    Tellen D. Bennett, MD, MS$^{1,2}$,
    Debashis Ghosh, PhD$^{1,3}$}

\institutes{
    $^1$CU Data Science to Patient Value (D2V), Anschutz Medical Campus, Aurora, CO;
    $^2$Pediatric Critical Care, University of Colorado School of Medicine, Aurora, CO;
    $^3$Biostatistics and Informatics, Colorado School of Public Health, Aurora, CO
}

\maketitle

\section*{Introduction}

In the natural language processing (NLP) machine learning domain, great success has been found recently through the creation of language models built in an unsupervised fashion. Derived from successes seen with transfer learning in domains such as imaging, one early publication \cite{howard_universal_2018} showed success with NLP transfer learning. Building upon this work primarily through the use of massive data sets and compute power \cite{radford_improving_2018,radford_language_2019} resulted in new records on various benchmark tests for state of the art NLP.

To date, the successes seen with transfer learning in NLP have focused on general English non-medical text. The contribution of this work is to demonstrate the effectiveness of the transfer learn techniques. In order to improve reproducibility and replicability, we have made all of our source code publicly available \footnote{\url{https://github.com/magic-lantern/nlp-transfer-learning}}.

\section*{Methods}

For this study, we have used the Medical Information Mart for Intensive Care (MIMIC) III \cite{johnson_mimic-iii_2016} data set which contains both clinical notes as well as a wide array of more structured data. Although the MIMIC dataset is comprised primarily of inpatient ICU stays, some outpatient notes are included. The patient population includes 38,597 distinct adult patients admitted between 2001 and 2012 as well as 7,870 neonates admitted between 2001 and 2008. Our cohort from MIMIC consisted of a random 10\% sample without replacement of clinical notes with different seeds for the language model vs classification tasks to reduce overlap. All experiments were carried out on a virtual machine with 6 virtual CPUs, 32GB RAM, 200GB of virtualized disk space, and an Nvidia Tesla P100 GPU with 16GB of GPU RAM.

We conducted several experiments to evaluate the usefulness of transfer learning in clinical NLP. The basis for transfer learning in this clinical NLP work is the use of a language model. NLP transfer learning is similar to transfer learning in computer vision where a computer vision general model can recognize general image features such as lines, circles, edges, and other patterns. In NLP transfer learning, a language model is trained to predict and recognize language features in terms of probabilities at both the word and sentence level \cite{Bengio:2003:NPL:944919.944966}. Language models are trained in an unsupervised fashion by having the model predict the next word in a sentence of text given previous words.

Our first experiment was to take an existing language model derived from Wikipedia articles (WT-103) \cite{Merity2016Sep} and retrain it with clinical text. The neural network architecture chosen was based on Averaged Stochastic Gradient Weight-Dropped LSTM (AWD-LSTM) \cite{Merity2017Aug}. Text was prepared by tokenization (identifying separate words, numbers, symbols) and then conversion of those tokens to a number. Special tokens are inserted into the sentences to preserve the original text structure such as capitalization, repeated words, and identification words not in the vocabulary list. No stemming, lemmatization, nor stop word removal was performed.

Once the language model was fine-tuned against a random sample of MIMIC clinical notes, we used the language model for 2 separate classification tasks. The first classification task was to predict the value stored in the DESCRIPTION field; in NOTEEVENTS, CATEGORY and DESCRIPTION are used to define the type of clinical note. The second classification task was to predict length of stay (ADMISSIONS.DISCHTIME - ADMISSIONS.ADMITTIME) for admitted patients with clinical notes, based on notes from their first day of stay, binned into 0 through 9 days and > 9 days. For comparison, we also performed the classification tasks with no language model fine tuning to compare accuracy over 10 epochs of training.

\section*{Results}

For the language model, we used 60,000 as the vocabulary size. There are 14,655 tokens that overlap between WT-103 language model and MIMIC language model; e.g. words such as 'circumference' and 'purple' exist in both. Words unique to the MIMIC language model include examples such as 'transgastric', 'dysregulation', and 'bronchiectasis'. Overall there is about 25\% overlap in vocabulary. With 1 frozen (adjusting weights of last layer only) and 10 unfrozen (adjusting weights of all layers) epochs, the model is goes from 55.9\% accuracy to 67.7\% accuracy at predicting the next word in a clinical note. The resulting model is about 550MB in size.

For the DESCRIPTION classifier, there was a 9.91\% (20,648 notes) overlap between notes used for training and validating the language model and training and validating the DESCRIPTION classifier. There are 1,708 distinct DESCRIPTION values present, with most being labeled generically as 'Report' (54.5\%), and a long tail of decreasing frequency including 695 DESCRIPTIONS that are only used once . When starting with a MIMIC fine-tuned language model accuracy goes from 90.7\% to 9X.X\%, and weighted F1 from 0.88 to 0.XX, with X epochs of training. By comparison, starting with a non-fine-tuned language model, accuracy goes from 86.5\% to XX.X\%, and weighted F1 from XX to XX with X epochs of training.

For the length of stay classifier... 

\section*{Discussion}

One of the benefits of this method over larger and deeper neural networks is that this work can be performed on a single GPU with training times for language model at about 12 hours and each of the classifiers being trained in under 5 hours. Increased accuracy and F1 could also be achieved through more training epochs. More advanced network architectures based around the Transformer\cite{vaswani_attention_2017} have shown impressive results but require much more training time and GPU resources. Future work in this area could include the use of bi-directional language models such as BERT\cite{Devlin2018Oct}.

\section*{Conclusion}

A few sentences here.

\bibliography{bib}
\bibliographystyle{vancouver}

\end{document}

