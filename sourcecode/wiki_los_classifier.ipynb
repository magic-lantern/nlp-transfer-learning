{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on general purpose language model, train a Length of Stay classifier\n",
    "\n",
    "Instead of building from a MIMIC trained language model, use the general purpose ULMFit Wiki trained model\n",
    "\n",
    "This notebook is mostly a copy of mimic_nlp_classifier_los.ipynb - but with visualizations and some debugging output removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import gc\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup filenames and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas doesn't understand ~, so provide full path\n",
    "base_path = Path.home() / 'mimic'\n",
    "\n",
    "# files used during processing - all aggregated here\n",
    "admissions_file = base_path/'ADMISSIONS.csv'\n",
    "notes_file = base_path/'NOTEEVENTS.csv'\n",
    "\n",
    "class_file = 'wiki_los_cl_data.pickle'\n",
    "notes_pickle_file = base_path/'noteevents.pickle'\n",
    "lm_file = 'cl_lm.pickle' # actual file is at base_path/lm_file but due to fastai function, have to pass file name separately\n",
    "init_model_file = base_path/'wiki_los_cl_head'\n",
    "cycles_file = base_path/'wiki_los_cl_num_iterations.pickle'\n",
    "enc_file = 'wiki_cl_enc'\n",
    "ft_file = 'wiki_los_cl_fine_tuned_'\n",
    "freeze_two = base_path/'wiki_los_cl_freeze_two'\n",
    "freeze_three = base_path/'wiki_los_cl_freeze_three'\n",
    "\n",
    "training_history_file = 'wiki_los_cl_history'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup parameters for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original data set too large to work with in reasonable time due to limted GPU resources\n",
    "pct_data_sample = 0.1\n",
    "# how much to hold out for validation\n",
    "valid_pct = 0.2\n",
    "# for repeatability - different seed than used with language model\n",
    "seed = 1776\n",
    "lm_seed = 42\n",
    "# for classifier, on unfrozen/full network training\n",
    "# batch size of 128 GPU uses ?? GB RAM\n",
    "# batch size of 96 GPU uses 22 GB RAM\n",
    "# batch size of 48 GPU uses GB RAM\n",
    "bs=96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this doesn't free memory, can restart Python kernel.\n",
    "# if that still doesn't work, try OS items mentioned here: https://docs.fast.ai/dev/gpu.html\n",
    "def release_mem():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "release_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = pd.DataFrame()\n",
    "if os.path.isfile(notes_pickle_file):\n",
    "    print('Loading noteevent pickle file')\n",
    "    orig_df = pd.read_pickle(notes_pickle_file)\n",
    "    print(orig_df.shape)\n",
    "else:\n",
    "    print('Could not find noteevent pickle file; creating it')\n",
    "    # run this the first time to covert CSV to Pickle file\n",
    "    orig_df = pd.read_csv(notes_file, low_memory=False, memory_map=True)\n",
    "    orig_df.to_pickle(notes_pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since seed is different, this should be quite different than the language model dataset.\n",
    "\n",
    "Should I show details on how many records are in language model dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_df = orig_df[orig_df.HADM_ID.notnull()].copy()\n",
    "notes_df.HADM_ID = notes_df.HADM_ID.astype(int)\n",
    "notes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading ADMISSIONS.csv')\n",
    "a_orig = pd.read_csv(admissions_file, low_memory=False, memory_map=True)\n",
    "a_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_df = a_orig[['HADM_ID', 'ADMITTIME', 'DISCHTIME']].copy()\n",
    "#pd.to_datetime('2014-04-09T152959.999993-0500', utc=True)\n",
    "# passing format just to make sure conversion doesn't mess something up\n",
    "a_df['admittime'] = pd.to_datetime(a_df.ADMITTIME, format='%Y-%m-%d %H:%M:%S')\n",
    "a_df['dischtime'] = pd.to_datetime(a_df.DISCHTIME, format='%Y-%m-%d %H:%M:%S')\n",
    "a_df['los'] = (a_df['dischtime'] - a_df['admittime']).astype('timedelta64[D]')\n",
    "# can't use a float in neural network\n",
    "a_df['los'] = a_df.los.astype(int)\n",
    "# there are 98 admissions where length of stay is negative. change to 0\n",
    "a_df.loc[a_df.los < 0, 'los'] = 0\n",
    "a_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we only keep rows with notes\n",
    "combined_df = pd.merge(a_df, notes_df, on='HADM_ID', how='right')\n",
    "\n",
    "# passing format just to make sure conversion doesn't mess something up\n",
    "combined_df['charttime'] = pd.to_datetime(combined_df.CHARTTIME, format='%Y-%m-%d %H:%M:%S')\n",
    "combined_df['chartdate'] = pd.to_datetime(combined_df.CHARTDATE, format='%Y-%m-%d')\n",
    "combined_df['admitdate'] = combined_df.admittime.dt.date\n",
    "combined_df = combined_df[['HADM_ID', 'ROW_ID', 'admittime', 'admitdate', 'dischtime', 'los', 'chartdate', 'charttime', 'TEXT']]\n",
    "combined_df.rename(columns={'HADM_ID': 'hadm_id', 'ROW_ID': 'row_id', 'TEXT': \"text\"}, inplace=True)\n",
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these should all be zero\n",
    "print(combined_df[combined_df.los.isnull()].shape)\n",
    "print(combined_df[combined_df.hadm_id.isnull()].shape)\n",
    "print(combined_df[combined_df.text.isnull()].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df.hadm_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Use notes from day 1 of stay to predict LOS\n",
    "\n",
    "\n",
    "    For each admission\n",
    "        do they have notes on day 1 of stay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build data set for LOS analysis\n",
    "\n",
    "First, find rows of data related to first day of stay\n",
    "\n",
    "Then, combine notes from first day into one text field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the slowest cell in pre-processing portion of the notebook \n",
    "fday = combined_df.groupby('hadm_id', as_index=False).apply(lambda g: g[\n",
    "    (g.charttime >= g.admittime) & (g.charttime < (g.admittime + pd.Timedelta(hours=24)))\n",
    "    |\n",
    "    (g.chartdate == g.admitdate)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = fday[['hadm_id', 'row_id']].reset_index(drop=True)\n",
    "tmp['row_id'] = tmp['row_id'].astype(str)\n",
    "tmp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_notes_row_ids = tmp.groupby(['hadm_id'], as_index=False).agg({\n",
    "    'row_id': lambda x: \",\".join(x)\n",
    "})\n",
    "combined_notes_row_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_fday = fday.groupby(['hadm_id', 'los'], as_index=False).agg({\n",
    "    'text': lambda x: \"\\n\\n\\n\\n\".join(x)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_fday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_fday.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_fday.los.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_fday.los.value_counts().head(10))\n",
    "print(combined_fday.los.value_counts().tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = combined_fday.los.value_counts()\n",
    "print('Number of records where length of stay is unique to that person:', len(s[s == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min\n",
    "print('Min LOS:', combined_fday.los.min())\n",
    "# max\n",
    "print('Max LOS:', combined_fday.los.max())\n",
    "# median\n",
    "print('Median LOS:', combined_fday.los.median())\n",
    "# mean\n",
    "print('Mean LOS:', combined_fday.los.mean())\n",
    "print('Mode LOS:', combined_fday.los.mode()[0]) # returns a series, just want the value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncate LOS to max of 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_fday = combined_fday.copy()\n",
    "trunc_fday.loc[trunc_fday.los > 9, 'los'] = 10\n",
    "trunc_fday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = trunc_fday.los.value_counts()\n",
    "len(s[s == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trunc_fday.los.value_counts().head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rowid_sample = combined_notes_row_ids.sample(frac=pct_data_sample, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = trunc_fday.sample(frac=pct_data_sample, random_state=seed)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be 5535 - 100% overlap\n",
    "len(set(rowid_sample.hadm_id.unique()) & set(df.hadm_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.hadm_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------- stats on 10% random sample ---------')\n",
    "print('Min LOS:', df.los.min())\n",
    "print('Max LOS:', df.los.max())\n",
    "print('Median LOS:', df.los.median())\n",
    "print('Mean LOS:', df.los.mean())\n",
    "print('Mode LOS:', df.los.mode()[0]) # returns a series, just want the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s.apply(pd.Series).stack().reset_index(drop=True)\n",
    "\n",
    "r = rowid_sample.groupby(['hadm_id'], as_index=False).agg({\n",
    "    'row_id': lambda x: x.str.split(',')\n",
    "})\n",
    "row_ids = r.row_id.apply(pd.Series).stack().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(row_ids.shape)         # 33,914\n",
    "print(len(row_ids.unique())) # 33,914"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare overlap between these notes and language model notes set\n",
    "lm_df = orig_df.sample(frac=pct_data_sample, random_state=42)\n",
    "print('rows in dataframe for NN:', len(row_ids.unique()))\n",
    "print('rows in language model:', len(lm_df.ROW_ID.unique()))\n",
    "print('row_ids in both:', len(set(row_ids.unique()) & set(lm_df.ROW_ID.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now for some Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What if LOS was a string? Would accuracy, memory, or training time change?\n",
    "#df.los = df.los.astype(int)\n",
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create language model without fine-tuning to MIMIC notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpfile = base_path/lm_file\n",
    "\n",
    "if os.path.isfile(tmpfile):\n",
    "    print('loading existing language model')\n",
    "    lm = load_data(base_path, lm_file, bs=bs)\n",
    "else:\n",
    "    print('creating new language model')\n",
    "    lm_df = orig_df.sample(frac=pct_data_sample, random_state=lm_seed)\n",
    "    lm = (TextList.from_df(lm_df, base_path, cols='TEXT')\n",
    "               #df has several columns; actual text is in column TEXT\n",
    "               .split_by_rand_pct(valid_pct=valid_pct, seed=lm_seed)\n",
    "               #We randomly split and keep 10% for validation\n",
    "               .label_for_lm()\n",
    "               #We want to do a language model so we label accordingly\n",
    "               .databunch(bs=bs))\n",
    "    lm.save(tmpfile)\n",
    "    print('completed creating new language model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(lm, AWD_LSTM, drop_mult=0.3)\n",
    "learn.save_encoder(enc_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is a very CPU and RAM intensive process - no GPU involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = base_path/class_file\n",
    "if os.path.isfile(filename):\n",
    "    data_cl = load_data(base_path, class_file, bs=bs)\n",
    "    print('loaded existing data bunch')\n",
    "else:\n",
    "    print('creating new data bunch')\n",
    "    data_cl = (TextList.from_df(df, base_path, cols='text', vocab=lm.vocab)\n",
    "               #df has several columns; actual text is in column TEXT\n",
    "               .split_by_rand_pct(valid_pct=valid_pct, seed=seed)\n",
    "               #We randomly split and keep 20% for validation, set seed for repeatability\n",
    "               .label_from_df(cols='los')\n",
    "               .databunch(bs=bs))\n",
    "    data_cl.save(filename)\n",
    "    print('created new data bunch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using weighted F1 to account for class imbalance\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_cl, AWD_LSTM, drop_mult=0.5, metrics=[accuracy, FBeta(average='weighted', beta=1)])\n",
    "learn.load_encoder(enc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with P100/P40 this takes maybe 5 minutes\n",
    "# with 2017 macbook pro, intel core i7 3.1Ghz, this takes about 160 minutes\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change learning rate based on results from the above plot.\n",
    "\n",
    "Next several cells test various learning rates to find ideal learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Train with selected learning rate\n",
    "\n",
    "Results from `learn.fit_one_cycle()`\n",
    "\n",
    "    Training new initial learner\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \tf_beta \ttime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(str(init_model_file) + '.pth'):\n",
    "    learn.load(init_model_file)\n",
    "    learn.load_encoder(enc_file)\n",
    "    print('loaded initial learner')\n",
    "else:\n",
    "    print('Training new initial learner')\n",
    "    learn.fit_one_cycle(1, 1e-1, moms=(0.8,0.7),\n",
    "                       callbacks=[\n",
    "                           callbacks.CSVLogger(learn, filename=training_history_file, append=True)\n",
    "                       ])\n",
    "    print('Saving new learner')\n",
    "    learn.save(init_model_file)\n",
    "    print('Finished generating new learner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Results from the freeze_two learner\n",
    "\n",
    "With `learn.fit_one_cycle()`\n",
    "\n",
    "    epoch \ttrain_loss \tvalid_loss \taccuracy \tf_beta \ttime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(str(freeze_two) + '.pth'):\n",
    "    learn.load(freeze_two)\n",
    "    print('loaded freeze_two learner')\n",
    "else:\n",
    "    print('Training new freeze_two learner')\n",
    "    learn.freeze_to(-2)\n",
    "    learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7),\n",
    "                       callbacks=[\n",
    "                           callbacks.CSVLogger(learn, filename=training_history_file, append=True)\n",
    "                       ])\n",
    "    print('Saving new freeze_two learner')\n",
    "    learn.save(freeze_two)\n",
    "    print('Finished generating new freeze_two learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(str(freeze_three) + '.pth'):\n",
    "    learn.load(freeze_three)\n",
    "    print('loaded freeze_three learner')\n",
    "else:\n",
    "    print('Training new freeze_three learner')\n",
    "    learn.freeze_to(-3)\n",
    "    learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7),\n",
    "                       callbacks=[\n",
    "                           callbacks.CSVLogger(learn, filename=training_history_file, append=True)\n",
    "                       ])\n",
    "    print('Saving new freeze_three learner')\n",
    "    learn.save(freeze_three)\n",
    "    print('Finished generating new freeze_three learner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(cycles_file):\n",
    "    with open(cycles_file, 'rb') as f:\n",
    "        prev_cycles = pickle.load(f)\n",
    "    print('This model has been trained for', prev_cycles, 'epochs already')  \n",
    "else:\n",
    "    prev_cycles = 0\n",
    "    print('This model NOT been trained yet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cycles = 7\n",
    "\n",
    "file = ft_file + str(prev_cycles)\n",
    "learner_file = base_path/file\n",
    "callback_save_file = str(learner_file) + '_auto'\n",
    "\n",
    "learn.fit_one_cycle(num_cycles, slice(1e-4/(2.6**4),1e-4), moms=(0.8,0.7),\n",
    "                    callbacks=[\n",
    "                        callbacks.SaveModelCallback(learn, every='epoch', monitor='accuracy', name=callback_save_file),\n",
    "                        # CSVLogger only logs when num_cycles are complete\n",
    "                        callbacks.CSVLogger(learn, filename=training_history_file, append=True)\n",
    "                    ])\n",
    "file = ft_file + str(prev_cycles + num_cycles)\n",
    "learner_file = base_path/file\n",
    "learn.save(learner_file)\n",
    "\n",
    "with open(cycles_file, 'wb') as f:\n",
    "    pickle.dump(num_cycles + prev_cycles, f)\n",
    "release_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
